{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c09bb69-35b4-4d76-9186-676d9463435e",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f4af1-df94-42fe-b8f0-7b790a3229a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/21/2022 22:04:01 - INFO - __main__ -   device: cuda, n_gpu: 1\n",
      "02/21/2022 22:04:08 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='codebase_ruby.jsonl', output_dir='./demo/python_model', eval_data_file='codebase_ruby.jsonl', test_data_file='codebase_ruby.jsonl', codebase_file='codebase.jsonl', model_name_or_path='microsoft/codebert-base', config_name='microsoft/codebert-base', tokenizer_name='microsoft/codebert-base', nl_length=128, code_length=256, do_train=False, do_eval=True, do_test=True, train_batch_size=32, eval_batch_size=64, learning_rate=2e-05, max_grad_norm=1.0, num_train_epochs=10, seed=123456, n_gpu=1, device='cuda:5')\n"
     ]
    }
   ],
   "source": [
    "!lang=ruby python run.py \\\n",
    "    --output_dir=./demo/python_model \\\n",
    "    --config_name=microsoft/codebert-base \\\n",
    "    --model_name_or_path=microsoft/codebert-base \\\n",
    "    --tokenizer_name=microsoft/codebert-base \\\n",
    "    --do_eval \\\n",
    "    --do_test \\\n",
    "    --train_data_file=codebase_ruby.jsonl \\\n",
    "    --eval_data_file=codebase_ruby.jsonl \\\n",
    "    --test_data_file=codebase_ruby.jsonl \\\n",
    "    --codebase_file=codebase.jsonl \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --code_length 256 \\\n",
    "    --nl_length 128 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --seed 123456 2>&1| tee saved_models/ruby/test.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "012f0cd2-2c99-4162-b891-77bba1b5c5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,2,3,4,5\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f05b056-9307-4fb0-8430-b8b6e6d4838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999cdf3c-adac-4418-8696-5e913aafdad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/22/2022 21:40:15 - INFO - run -   device: cuda, n_gpu: 5\n",
      "02/22/2022 21:40:22 - INFO - run -   Training/evaluation parameters <__main__.Args object at 0x7f2f3b4e97f0>\n"
     ]
    }
   ],
   "source": [
    "class Args():\n",
    "    do_eval = True\n",
    "    seed = 123456\n",
    "    config_name = 'microsoft/codebert-base'\n",
    "    tokenizer_name = 'microsoft/codebert-base'\n",
    "    model_name_or_path = 'microsoft/codebert-base'\n",
    "    do_train = False\n",
    "    do_eval  = True\n",
    "    do_test = False\n",
    "    output_dir=\"./demo/python_model\"\n",
    "    eval_data_file=\"codebase_ruby.jsonl\"\n",
    "    code_length=256\n",
    "    nl_length=128\n",
    "    eval_batch_size=64\n",
    "    codebase_file='codebase_ruby.jsonl'\n",
    "    #set log\n",
    "    \n",
    "args = Args()\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                datefmt='%m/%d/%Y %H:%M:%S',level=logging.INFO )\n",
    "#set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = 'cuda'\n",
    "logger.info(\"device: %s, n_gpu: %s\",device, args.n_gpu)\n",
    "\n",
    "# Set seed\n",
    "set_seed(args.seed)\n",
    "\n",
    "#build model\n",
    "config = RobertaConfig.from_pretrained(args.config_name if args.config_name else args.model_name_or_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(args.tokenizer_name)\n",
    "model = RobertaModel.from_pretrained(args.model_name_or_path)\n",
    "model=Model(model)\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "model.to(args.device)\n",
    "\n",
    "# Training\n",
    "if args.do_train:\n",
    "    train(args, model, tokenizer)\n",
    "\n",
    "# Evaluation\n",
    "results = {}\n",
    "if args.do_eval:\n",
    "    checkpoint_prefix = 'pytorch_model.bin'\n",
    "    output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
    "    model.load_state_dict(torch.load(output_dir),strict=False)\n",
    "    model.to(args.device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc6d4a-c8c5-48a2-87ed-fe72dcd3cdb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# result=evaluate(args, model, tokenizer,args.eval_data_file)\n",
    "# logger.info(\"***** Eval results *****\")\n",
    "# for key in sorted(result.keys()):\n",
    "#     logger.info(\"  %s = %s\", key, str(round(result[key],4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b967130-ffdc-4fb7-89f6-31ac6a5b8506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/22/2022 21:40:24 - INFO - run -   ***** Starting evaluation *****\n"
     ]
    }
   ],
   "source": [
    "#def evaluate(args, model, tokenizer,file_name,eval_when_training=False):\n",
    "file_name=args.eval_data_file\n",
    "logger.info(\"***** Starting evaluation *****\")\n",
    "query_dataset = TextDataset(tokenizer, args, 'touf/test.jsonl')\n",
    "query_sampler = SequentialSampler(query_dataset)\n",
    "query_dataloader = DataLoader(query_dataset, sampler=query_sampler, batch_size=args.eval_batch_size,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4470049c-96e7-4866-9aec-56603237faa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/22/2022 21:43:33 - INFO - run -   ***** Loaded data for evaluation *****\n"
     ]
    }
   ],
   "source": [
    "code_dataset = TextDataset(tokenizer, args, 'touf/codebase.jsonl')\n",
    "code_sampler = SequentialSampler(code_dataset)\n",
    "code_dataloader = DataLoader(code_dataset, sampler=code_sampler, batch_size=args.eval_batch_size,num_workers=4)\n",
    "logger.info(\"***** Loaded data for evaluation *****\")\n",
    "# multi-gpu evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e05b6f7-30a6-4371-ba28-aa9a858ce22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/22/2022 21:43:33 - INFO - run -   ***** Running evaluation *****\n",
      "02/22/2022 21:43:33 - INFO - run -     Num queries = 52561\n",
      "02/22/2022 21:43:33 - INFO - run -     Num codes = 183295\n",
      "02/22/2022 21:43:33 - INFO - run -     Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "if args.n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Eval!\n",
    "logger.info(\"***** Running evaluation *****\")\n",
    "logger.info(\"  Num queries = %d\", len(query_dataset))\n",
    "logger.info(\"  Num codes = %d\", len(code_dataset))\n",
    "logger.info(\"  Batch size = %d\", args.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff3333dd-0e81-4841-b57a-6036804653b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dff6e56-4406-49b0-8a72-9048307613e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code_vecs=[] ?\n",
    "# nl_vecs=[]\n",
    "nl_vecs_t= torch.empty((0,768), device=args.device)\n",
    "code_vecs_t=torch.empty((0,768), device=args.device)\n",
    "for batch in query_dataloader:  \n",
    "    nl_inputs = batch[1].to(args.device)\n",
    "    with torch.no_grad():\n",
    "        nl_vec = model(nl_inputs=nl_inputs)\n",
    "        # nl_vecs.append(nl_vec.cpu().numpy()) \n",
    "        nl_vecs_t = torch.cat((nl_vecs_t, nl_vec),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b626f1-749f-44f9-b37c-0f0625abd07a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in code_dataloader:\n",
    "    code_inputs = batch[0].to(args.device)    \n",
    "    with torch.no_grad():\n",
    "        code_vec= model(code_inputs=code_inputs)\n",
    "        # code_vecs.append(code_vec.cpu().numpy())  \n",
    "        code_vecs_t = torch.cat((code_vecs_t, code_vec),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c2c88f3-7d1e-4370-9a30-81575d743ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bd1c57a-7cca-4e11-a134-5acd10c9795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_vecs_t = code_vecs_t.to('cuda:3')\n",
    "nl_vecs_t = nl_vecs_t.to('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e8191a91-e4eb-42ae-b2dc-d3b38780464b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'code_vecs_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [87]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcode_vecs_t\u001b[49m\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[0;31mNameError\u001b[0m: name 'code_vecs_t' is not defined"
     ]
    }
   ],
   "source": [
    "code_vecs_t.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "69b5a302-712a-4765-9a74-c1fa60b44d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce2fc19d-eb28-43d3-923f-dd2517c1d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(code_vecs_t, 'code_vecs_t.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25d6474b-e112-400e-a891-bfaa57492cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nl_vecs_t, 'nl_vecs_t.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ab4683f-d70e-445a-8b49-82f8f3fe6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_chunks = torch.chunk(nl_vecs_t, 2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88cbd9d2-361b-4a6c-8f85-f94936bc08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_chunks = torch.chunk(code_vecs_t, 2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac1dccef-1ec1-4d37-be1e-b58c43f4ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_vecs_t2 = code_chunks[1].to('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f5d84e8-c6ef-47ad-8dd1-699741dde894",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_chunks_t2 = nl_chunks[1].to('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3e41470-955a-4205-94fa-67c61500f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_vecs_t1  = code_chunks[0].to('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "411d17ca-1f46-4598-85e5-797ac17e3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_chunks_t1 = nl_chunks[0].to('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ab44161-0b15-4f3d-aa76-910abee3b2c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nl_vecs_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m nl_vecs_t\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m code_vecs_t\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nl_vecs_t' is not defined"
     ]
    }
   ],
   "source": [
    "del nl_vecs_t\n",
    "del code_vecs_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "60f741f7-424c-4e6c-9964-447868d81692",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'code_vecs_t1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [81]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_vecs_t1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnl_chunks_t1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores_t1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'code_vecs_t1' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"{code_vecs_t1}\")\n",
    "print(f\"{nl_chunks_t1}\")\n",
    "print(f\"{scores_t1}\")\n",
    "print(f\"{code_vecs_t2}\")\n",
    "print(f\"{nl_chunks_t2}\")\n",
    "print(f\"{scores_t2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0311a740-b5a1-4b34-8d30-b355e95674fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scores_t1\n",
    "del scores_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3cf0dc36-c7dd-4290-bd49-e2df5f205bd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nl_chunks_t2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# code_vecs=np.concatenate(code_vecs,0)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# nl_vecs=np.concatenate(nl_vecs,0)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# scores=np.matmul(nl_vecs,code_vecs.T)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print(f\"code_vecs_t shape: {code_vecs_t.shape}\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# scores_t1 = torch.matmul(nl_chunks_t1,code_vecs_t1.T)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m scores_t2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[43mnl_chunks_t2\u001b[49m,code_vecs_t2\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(scores_t1\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(scores_t2\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nl_chunks_t2' is not defined"
     ]
    }
   ],
   "source": [
    "# code_vecs=np.concatenate(code_vecs,0)\n",
    "# nl_vecs=np.concatenate(nl_vecs,0)\n",
    "# scores=np.matmul(nl_vecs,code_vecs.T)\n",
    "# sort_ids=np.argsort(scores, axis=-1, kind='quicksort', order=None)[:,::-1]   \n",
    "\n",
    "# print(f\"Nl_vecs_t size: {nl_vecs_t.element_size() * nl_vecs_t.nelement()}\")\n",
    "# print(f\"Nl_vecs_t elem size: {nl_vecs_t.element_size()}\")\n",
    "# print(f\"Nl_vecs_t n elem: {nl_vecs_t.nelement()}\")\n",
    "# print(f\"code_vecs_t size: {code_vecs_t.element_size() * code_vecs_t.nelement()}\")\n",
    "# print(f\"nl_vecs shape: {nl_vecs_t.shape}\")\n",
    "# print(f\"code_vecs_t shape: {code_vecs_t.shape}\")\n",
    "# scores_t1 = torch.matmul(nl_chunks_t1,code_vecs_t1.T)\n",
    "scores_t2 = torch.matmul(nl_chunks_t2,code_vecs_t2.T)\n",
    "print(scores_t1.device)\n",
    "print(scores_t2.device)\n",
    "# del nl_chunks_t1\n",
    "# del nl_chunks_t2\n",
    "# del code_vecs_t1\n",
    "# del code_vecs_t2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5fd23897-752c-4997-a37a-3a04ef622acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 7         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  707964 KB |    1704 MB |   16141 GB |   16140 GB |\n",
      "|       from large pool |  707581 KB |    1704 MB |   16137 GB |   16136 GB |\n",
      "|       from small pool |     383 KB |       2 MB |       4 GB |       4 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  707964 KB |    1704 MB |   16141 GB |   16140 GB |\n",
      "|       from large pool |  707581 KB |    1704 MB |   16137 GB |   16136 GB |\n",
      "|       from small pool |     383 KB |       2 MB |       4 GB |       4 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  710656 KB |   23328 MB |  145826 MB |  145132 MB |\n",
      "|       from large pool |  708608 KB |   23326 MB |  145822 MB |  145130 MB |\n",
      "|       from small pool |    2048 KB |       4 MB |       4 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    2692 KB |  574530 KB |   13356 GB |   13356 GB |\n",
      "|       from large pool |    1027 KB |  573376 KB |   13352 GB |   13352 GB |\n",
      "|       from small pool |    1665 KB |    3351 KB |       4 GB |       4 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       6    |     218    |    1249 K  |    1249 K  |\n",
      "|       from large pool |       2    |      86    |     995 K  |     995 K  |\n",
      "|       from small pool |       4    |     137    |     254 K  |     254 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       6    |     218    |    1249 K  |    1249 K  |\n",
      "|       from large pool |       2    |      86    |     995 K  |     995 K  |\n",
      "|       from small pool |       4    |     137    |     254 K  |     254 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       3    |     220    |     546    |     543    |\n",
      "|       from large pool |       2    |     218    |     544    |     542    |\n",
      "|       from small pool |       1    |       2    |       2    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       4    |      29    |  548726    |  548722    |\n",
      "|       from large pool |       1    |      24    |  501964    |  501963    |\n",
      "|       from small pool |       3    |       9    |   46762    |   46759    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "62486e33-ab70-46e2-8a15-d98d334f097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scores_t1\n",
    "del scores_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fd9ab99f-13be-4cb3-9a72-3c465672efdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 17.95 GiB (GPU 3; 23.65 GiB total capacity; 19.29 GiB already allocated; 3.49 GiB free; 19.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [80]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#scores_t.to(torch.device('cuda:3'))\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#del nl_vecs_t\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#del code_vecs_t\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# sort_ids_t1=torch.flip(torch.argsort(scores_t1, axis=-1),dims=(1,))\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m sort_ids_t2\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mflip(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores_t2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m,dims\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 17.95 GiB (GPU 3; 23.65 GiB total capacity; 19.29 GiB already allocated; 3.49 GiB free; 19.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#scores_t.to(torch.device('cuda:3'))\n",
    "#del nl_vecs_t\n",
    "#del code_vecs_t\n",
    "sort_ids_t1=torch.flip(torch.argsort(scores_t1, axis=-1),dims=(1,))\n",
    "sort_ids_t2=torch.flip(torch.argsort(scores_t2, axis=-1),dims=(1,))\n",
    "#del scores_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d3b674bf-9eb2-4f28-a0c4-02e444575995",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ce8cb73a-e319-4088-8be8-5b27e5fd7d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(scores_t2, 'scores_t2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b832a81a-11ae-4b45-ace8-2ed5e8abb21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW\t Args\t CrossEntropyLoss\t DataLoader\t Dataset\t InputFeatures\t MSELoss\t Model\t RandomSampler\t \n",
      "RobertaConfig\t RobertaModel\t RobertaTokenizer\t SequentialSampler\t TensorDataset\t TextDataset\t WEIGHTS_NAME\t argparse\t args\t \n",
      "batch\t checkpoint_prefix\t code_chunks\t code_dataloader\t code_dataset\t code_inputs\t code_sampler\t code_vec\t config\t \n",
      "convert_examples_to_features\t device\t evaluate\t file_name\t get_linear_schedule_with_warmup\t json\t logger\t logging\t main\t \n",
      "nl_inputs\t nl_vec\t np\t os\t output_dir\t pickle\t query_dataloader\t query_dataset\t query_sampler\t \n",
      "random\t results\t set_seed\t tokenizer\t torch\t train\t \n"
     ]
    }
   ],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5cf115b-161d-457b-900d-a6d8e29c1ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nl_urls=[]\n",
    "code_urls=[]\n",
    "for example in query_dataset.examples:\n",
    "    nl_urls.append(example.url)\n",
    "\n",
    "for example in code_dataset.examples:\n",
    "    code_urls.append(example.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "69613f47-c6c3-48da-967c-59eb42e76e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ranks=[]\n",
    "for url, sort_id in zip(nl_urls,sort_ids_t.cpu().numpy()):\n",
    "    rank=0\n",
    "    find=False\n",
    "    for idx in sort_id[:1000]: # if code_url matches nl_url, rank +1\n",
    "        if find is False:\n",
    "            rank+=1\n",
    "        if code_urls[idx]==url:\n",
    "            find=True\n",
    "    if find:\n",
    "        ranks.append(1/rank)\n",
    "    else:\n",
    "        ranks.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c65b32e-8e05-4507-afdc-59b0111396e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mrr(indices, targets): #Mean Receiprocal Rank --> Average of rank of next item in the session.\n",
    "    \"\"\"\n",
    "    Calculates the MRR score for the given predictions and targets\n",
    "    Args:\n",
    "        indices (Bxk): torch.LongTensor. top-k indices predicted by the model.\n",
    "        targets (B): torch.LongTensor. actual target indices.\n",
    "    Returns:\n",
    "        mrr (float): the mrr score\n",
    "    \"\"\"\n",
    "    tmp = targets.view(-1, 1)#.squeeze()\n",
    "    # print(tmp)\n",
    "    targets = tmp.expand_as(indices)\n",
    "    hits = (targets == indices).nonzero()\n",
    "    ranks = hits[:, -1] + 1\n",
    "    ranks = ranks.float()\n",
    "    rranks = torch.reciprocal(ranks)\n",
    "    mrr = torch.sum(rranks).data / targets.size(0)\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "383683dc-6683-4d94-9772-4ef1af83e8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'code_ids', 'code_tokens', 'nl_ids', 'nl_tokens', 'uid', 'url']\n",
      "[0, 9232, 120, 1215, 34728, 1215, 1409, 1215, 13650, 36, 1651, 2156, 1546, 31723, 4839, 898, 5457, 38074, 1651, 646, 4832, 4135, 11655, 27779, 479, 349, 109, 1721, 1546, 1721, 114, 1546, 646, 321, 27779, 479, 159, 11173, 45994, 1546, 31723, 479, 159, 11173, 898, 5457, 120, 1215, 34728, 36, 1546, 646, 112, 27779, 4839, 253, 253, 898, 253, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "['<s>', 'def', 'Ġget', '_', 'network', '_', 'by', '_', 'name', 'Ġ(', 'Ġorganization', 'Ġ,', 'Ġnetwork', 'Name', 'Ġ)', 'Ġresult', 'Ġ=', 'Ġnil', 'Ġorganization', 'Ġ[', 'Ġ:', 'net', 'works', 'Ġ]', 'Ġ.', 'Ġeach', 'Ġdo', 'Ġ|', 'Ġnetwork', 'Ġ|', 'Ġif', 'Ġnetwork', 'Ġ[', 'Ġ0', 'Ġ]', 'Ġ.', 'Ġdown', 'case', 'Ġ==', 'Ġnetwork', 'Name', 'Ġ.', 'Ġdown', 'case', 'Ġresult', 'Ġ=', 'Ġget', '_', 'network', 'Ġ(', 'Ġnetwork', 'Ġ[', 'Ġ1', 'Ġ]', 'Ġ)', 'Ġend', 'Ġend', 'Ġresult', 'Ġend', '</s>']\n",
      "[0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "['<s>', '</s>']\n",
      "https://github.com/vmware/vcloud-rest/blob/45425d93acf988946cac375b803100ad01206459/lib/vcloud-rest/vcloud/network.rb#L54-L64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for example in query_dataset.examples:\n",
    "    print(dir(example))\n",
    "    print(example.code_ids)\n",
    "    print(example.code_tokens)\n",
    "    print(example.nl_ids)\n",
    "    print(example.nl_tokens)\n",
    "    print(example.url)\n",
    "    print(example.uid)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "612c79d2-8348-408d-9811-34daea0b4903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qdataset = {ex.uid:ex.url for ex in query_dataset.examples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3a7227e-bd29-4037-8460-90313accbde8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cdataset = {ex.uid:ex.url for ex in code_dataset.examples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "640a7507-fbeb-492f-8186-4c5b2f95a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [ex.uid for ex in query_dataset.examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fabecb86-2ff4-4c91-a53a-f045aeaf4af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52561"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_dataset.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689bddd4-add8-47e6-bc7b-32b188588d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_urls=[]\n",
    "code_urls=[]\n",
    "for example in query_dataset.examples:\n",
    "    nl_urls.append(example.uid)\n",
    "\n",
    "# for example in code_dataset.examples:\n",
    "#     code_urls.append(example.uid)\n",
    "\n",
    "ranks=[]\n",
    "tranks=[]\n",
    "ids_cpu = sort_ids_t\n",
    "nl_cpu = torch.arange(len(query_dataset.examples), device=ids_cpu)\n",
    "print(get_mrr(ids_cpu[:,:1000], nl_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e54b4090-26e6-4234-bf1e-681d4bd52e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4360, 4360])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_cpu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0712a617-f941-4171-82e1-00bf1c8d2a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001716851114805125"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for query_id, sort_id in zip(nl_urls,sort_ids_t.cpu().numpy()):\n",
    "    # ranks.append())\n",
    "    rank=0\n",
    "    find=False\n",
    "    for idx in sort_id[:1000]: # if code_url matches nl_url, rank +1\n",
    "        if find is False:\n",
    "            rank+=1\n",
    "        if code_urls[idx] == query_id: #if code_urls[idx]==query_id:\n",
    "            find=True\n",
    "    if find:\n",
    "        ranks.append(1/rank)\n",
    "    else:\n",
    "        ranks.append(0)\n",
    "np.mean(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7379845f-a3b1-4d7e-8e1c-d3fa6aae4988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([4360])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43249/389760503.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tmp = torch.tensor(indices).view(-1, 1)#.squeeze()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.LongTensor{[4360, 1]}, size=[4360]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [158]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m tmp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(indices)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#.squeeze()\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(tmp) \u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m targetss \u001b[38;5;241m=\u001b[39m \u001b[43mtmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m hits \u001b[38;5;241m=\u001b[39m (targetss \u001b[38;5;241m==\u001b[39m indices)\u001b[38;5;241m.\u001b[39mnonzero()\n\u001b[1;32m     19\u001b[0m ranks \u001b[38;5;241m=\u001b[39m hits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.LongTensor{[4360, 1]}, size=[4360]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)"
     ]
    }
   ],
   "source": [
    "nl_urls=[]\n",
    "code_urls=[]\n",
    "for example in query_dataset.examples:\n",
    "    nl_ids.append(example.url)\n",
    "for example in code_dataset.examples:\n",
    "    code_urls.append(example.url)\n",
    "\n",
    "sorted_ids = sort_ids_t.cpu()\n",
    "\n",
    "mrrs = []\n",
    "for targets, indices in zip(nl_ids,sorted_ids):\n",
    "    targets = torch.tensor(targets)#.squeeze()\n",
    "    print(targets.shape)\n",
    "    print(indices.shape)\n",
    "    tmp = torch.tensor(indices).view(-1, 1)#.squeeze()\n",
    "    # print(tmp)\n",
    "    targetss = tmp.expand_as(indices)\n",
    "    hits = (targetss == indices).nonzero()\n",
    "    ranks = hits[:, -1] + 1\n",
    "    ranks = ranks.float()\n",
    "    rranks = torch.reciprocal(ranks)\n",
    "    mrr = torch.sum(rranks).data / targets.size(0)\n",
    "    mrrs.append(mrr)\n",
    "    # print(get_mrr(f[:128], ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "544adba0-8dde-4dac-8de4-2611c82976a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"eval_mrr\":float(np.mean(mrrs))\n",
    "}\n",
    "result\n",
    "#return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
